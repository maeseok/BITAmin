{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39437b68",
   "metadata": {},
   "source": [
    "# **4조**\n",
    "**오차역전파(계산 그래프, 연쇄법칙, 역전파)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3c3ca7",
   "metadata": {},
   "source": [
    "## 1. 계산 그래프 및 연쇄법칙 (15점)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09ce25c",
   "metadata": {},
   "source": [
    "문제 1 : Local Gradient, Global Gradient가 무엇인지 서술하세요. (5점)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c40ea16",
   "metadata": {},
   "source": [
    "정답 : Local Gradient는 모델의 특정 파라미터에 대한 손실 함수의 기울기를 의미한다. \n",
    "Global Gradient는 모델의 모든 파라미터에 대한 손실 함수의 기울기를 통합하여 계산한 값이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b42f963",
   "metadata": {},
   "source": [
    "문제 2 : 역전파 과정에서는 <>를 활용하여 <>, <>를 곱해 최종 기울기를 계산합니다. <>에 들어갈 단어를 순서대로 적어주세요. (5점)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67224683",
   "metadata": {},
   "source": [
    "정답 : 체인 룰 , 국소 기울기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fc6ae9",
   "metadata": {},
   "source": [
    "문제 3 : 계산 그래프에서 각각의 노드가 순전파와 역전파에서 어떠한 역할을 하는지 전반적인 흐름을 서술하세요. (5점)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8558f0",
   "metadata": {},
   "source": [
    "정답 : 순전파에선 입력 데이터를 입력 노드로 전달 후에 각 연산 노드는 입력을 처리하여 결과를 계산한다. 그 결과를 다음 노드로 전달하여 마지막에 최종 출력을 계산한다.\n",
    "역전파에선 출력 노드에서 손실 함수의 기울기를 계산한다.\n",
    "각 연산 노드는 체인 룰을 이용해 기울기를 계산 후 이를 이전 노드로 전달한다.\n",
    "기울기는 네트워크를 거꾸로 따라 전파되어, 각 노드가 기울기를 계산하고\n",
    "이를 통해 파라미터를 업데이트할 준비를 한ㄷ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5021ef",
   "metadata": {},
   "source": [
    "## 2. 역전파 (35점)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35a0d6a",
   "metadata": {},
   "source": [
    "문제 1 : 역전파 알고리즘의 작동 방식은 손실함수의 오차값 (<> - <>)을 계산하여 왼쪽으로 전파해가며 각 노드가 갖고 있는 <> 값을 갱신해가는 과정입니다. <>에 들어갈 단어를 순서대로 적어주세요. (5점)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6aeedb",
   "metadata": {},
   "source": [
    "정답 : 예측값, 실제값, 기울기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27e5f55",
   "metadata": {},
   "source": [
    "문제 2 : 하단의 코드는 간단한 계산 그래프를 구축하여 순전파와 역전파를 구현하는 클래스들이며, 각각의 클래스는 곱셈과 덧셈을 수행합니다. 계산 그래프의 동작 방식을 살펴보기 위해 첫 번째 셀의 코드를 채우고, 두 번째 셀의 코드를 실행해주세요. (10점)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbd5d716",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y  \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x \n",
    "        return dx, dy\n",
    "\n",
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = x + y  \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout \n",
    "        dy = dout  \n",
    "        return dx, dy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "876cd804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사과 가격에 대한 미분: 2.2, 사과 개수에 대한 미분: 110\n",
      "귤 가격에 대한 미분: 3.3, 귤 개수에 대한 미분: 165\n",
      "소비세에 대한 미분: 650\n"
     ]
    }
   ],
   "source": [
    "# 코드 수정없이 실행\n",
    "\n",
    "# 입력 값 설정\n",
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# 계층 생성\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# 순전파\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)\n",
    "total_price = add_apple_orange_layer.forward(apple_price, orange_price)\n",
    "price_with_tax = mul_tax_layer.forward(total_price, tax)\n",
    "\n",
    "# 역전파\n",
    "dprice_with_tax = 1\n",
    "dtotal_price, dtax = mul_tax_layer.backward(dprice_with_tax)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dtotal_price)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(f\"사과 가격에 대한 미분: {dapple:.1f}, 사과 개수에 대한 미분: {dapple_num:.0f}\")\n",
    "print(f\"귤 가격에 대한 미분: {dorange:.1f}, 귤 개수에 대한 미분: {dorange_num:.0f}\")\n",
    "print(f\"소비세에 대한 미분: {dtax:.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47312242",
   "metadata": {},
   "source": [
    "문제 3 : 세션 때 배운 내용을 토대로 세 번째 셀의 코드를 채워주세요. (10점)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e0baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리가 없을 경우 실행\n",
    "\n",
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc3fd84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\LWG\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\LWG\\AppData\\Roaming\\Python\\Python312\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\LWG\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\LWG\\AppData\\Roaming\\Python\\Python312\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Python312\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Python312\\Lib\\asyncio\\base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Python312\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\LWG\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\LWG\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\LWG\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\LWG\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\LWG\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\LWG\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\LWG\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\LWG\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\LWG\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\LWG\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\LWG\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\LWG\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\LWG\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_28176\\1879024384.py\", line 12, in <module>\n",
      "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_28176\\1879024384.py:12: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
     ]
    }
   ],
   "source": [
    "# 코드 수정없이 실행\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "input_size = 10\n",
    "num_classes = 2\n",
    "batch_size = 4\n",
    "num_epochs = 5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 임의의 데이터와 레이블 생성\n",
    "data = torch.randn(100, input_size)\n",
    "labels = torch.randint(0, num_classes, (100,))\n",
    "dataset = TensorDataset(data, labels)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 간단한 RNN 모델 정의\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), hidden_size).to(device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "hidden_size = 20\n",
    "rnn_model = SimpleRNN(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# 손실 함수와 최적화 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80734f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7077111601829529\n",
      "Epoch 2, Loss: 0.698150520324707\n",
      "Epoch 3, Loss: 0.690264413356781\n",
      "Epoch 4, Loss: 0.6834771823883057\n",
      "Epoch 5, Loss: 0.680760223865509\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "for epoch in range(num_epochs):\n",
    "    rnn_model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # 입력 데이터를 3D 형태로 변환 (batch_size, seq_len, input_size)\n",
    "        images = images.unsqueeze(1) \n",
    "        # 기울기 초기화\n",
    "        optimizer.zero_grad()\n",
    "        # 순전파  \n",
    "        outputs = rnn_model(images)\n",
    "        # 손실 계산  \n",
    "        loss = criterion(outputs, labels)\n",
    "        # 역전파  \n",
    "        loss.backward()\n",
    "        # 최적화  \n",
    "        optimizer.step()  \n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2f86d8",
   "metadata": {},
   "source": [
    "문제 4 : 지금까지의 세션에서의 배운 내용을 토대로 Gradient와 Jacobian의 사용 예시를 각각 서술해주세요. (10점)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23394687",
   "metadata": {},
   "source": [
    "정답 : Gradient는 경량화된 Gradient Descent에서 사용할 수 있고, \n",
    "Jacobian은 다변량 함수의 민감도 분석에서 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96924e78",
   "metadata": {},
   "source": [
    "# **5조**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a747b8",
   "metadata": {
    "id": "nCG1caayueb3"
   },
   "source": [
    "# 1. 활성화 함수 이론 및 구현 코드에 대한 문제입니다.(25점)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4668c577",
   "metadata": {
    "id": "nYABXk0CvKO-"
   },
   "source": [
    "### 1-1. Binary Step Function, Linear Activation Function, Non-Linear Activation Function의 개념과 Non-Linear Activation Function를 사용하는 이유를 간단하게 작성해주세요.(3점)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2bd08b",
   "metadata": {
    "id": "1lG26ZWBF4hp"
   },
   "source": [
    "답)\n",
    "\n",
    "Binary Step Function: Binary Step Function은 입력이 특정 임계값(threshold)을 초과하면 1을 반환하고, 그렇지 않으면 0을 반환하는 함수이다.\n",
    "\n",
    "Linear Activation Function:  Linear Activation Function은 입력을 그대로 출력으로 반환하는 함수이다.\n",
    "\n",
    "Non-Linear Activation Function: Non-Linear Activation Function은 입력에 비선형 변환을 적용하여 출력하는 함수입니다. 예를 들어, ReLU(Rectified Linear Unit), Sigmoid, Tanh 등이 있다.\n",
    "\n",
    "Non-Linear Activation Function를 사용하는 이유 : 비선형 활성화 함수는 네트워크가 입력과 출력 사이의 비선형 관계를 학습할 수 있게 하고, 이는 단순 선형 함수로는 표현할 수 없는 복잡한 패턴을 모델링할 수 있게 해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb39e299",
   "metadata": {
    "id": "jeJOICIfGE1F"
   },
   "source": [
    "### 1-2. ???를 작성해주세요. (3점)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a1b35c",
   "metadata": {
    "id": "JifLcf9ZGEwt"
   },
   "source": [
    "sigmoid 함수 : 출력이 0과 1사이로 나와 층을 깊이 쌓으면 gradient vanising 문제가 있고 computational cost가 높습니다.\n",
    "\n",
    "\n",
    "Tanh 함수 : [-1, 1]범위로 shift된 형태를 띄며, 층이 쌓일수록 gradient 값이 0에 수렴하여 학습 효율이 낮아진다는 특징이 있습니다.\n",
    "\n",
    "\n",
    "ReLU 함수 : 현재 hidden layer의 activation function로 가장 많이 사용되며 간단하고 계산이 효율적인 함수입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0a2cd8",
   "metadata": {
    "id": "ocw0lFv-GM7O"
   },
   "source": [
    "### 1-3. Relu 계층을 구현한 코드입니다. 코드를 완성해주세요.(5점)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff43d26a",
   "metadata": {
    "id": "LAGC7nsrGMqV"
   },
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee418dc",
   "metadata": {
    "id": "XcRN0InyGMj7"
   },
   "source": [
    "### 1-4. 위 코드를 보고 forward 메서드에서 self.mask의 역할을 설명하고, 이를 통해 어떻게 ReLU 활성화 함수가 작동하는지 설명하세요.(3점)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766d3970",
   "metadata": {
    "id": "pSJJ53XTGWCC"
   },
   "source": [
    "답) 입력값 x의 각 요소가 0 이하인지를 나타내는 불리언 배열이다. (x <= 0) 코드에서 self.mask는 x의 요소 중 0 이하인 위치에 True, 0보다 큰 위치에 False 값을 갖게 된다. 이를 통해 입력값이 0 이하였던 위치에서는 gradient가 전달되지 않게 되고, 나머지 위치에서는 gradient가 그대로 전달된다. 이는 ReLU의 미분 특성을 반영한 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651118f7",
   "metadata": {
    "id": "TS1UtcBgGdGc"
   },
   "source": [
    "### 1-5. 위 코드를 통해 backward 메서드에서 dout[self.mask] = 0 코드는 어떤 역할을 하는지 설명하세요.(3점)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459943db",
   "metadata": {
    "id": "UO7AjLzHGMFg"
   },
   "source": [
    "답) dout[self.mask] = 0 코드는 ReLU 함수의 미분 특성을 반영하여, 입력값이 0 이하인 위치의 gradient를 0으로 만든다.\n",
    "이를 통해 역전파 과정에서 해당 위치의 gradient가 전파되지 않도록 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc17777",
   "metadata": {
    "id": "RNlr_fICGe3H"
   },
   "source": [
    "### 1-6. 아래는 PyTorch 기반 DenseBlock로 표현한 Relu 함수 코드입니다. Relu 함수에서는 Parameters로 'inplace'가 사용될 수 있는데, 'inplace = True'일 때 특징을 간단하게 설명해주세요.(3점)\n",
    "\n",
    "```\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "\tdef __init__(self, in_dim, out_dim):\n",
    "\t\tsuper(DenseBlock, self).__init__()\n",
    "\t\tself.dense = nn.Linear(in_dim, out_dim)\n",
    "\t\tself.act = nn.ReLU()\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tout = self.act(self.dense(x))\n",
    "\t\treturn out\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db2768c",
   "metadata": {
    "id": "JDud7fjuGewl"
   },
   "source": [
    "답) inplace=True는 입력 텐서를 직접 수정하여 메모리를 절약하고 약간의 속도 향상을 가져올 수 있다.\n",
    "그러나 이는 입력 값의 손실 및 역전파 중 그래디언트 계산 문제를 초래할 수 있기에\n",
    "주의 깊은 사용이 필요하며, 모델의 다른 부분이나 학습 과정에 영향을 미치지 않도록 신중하게 사용해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4aca7b",
   "metadata": {
    "id": "QMdl1dt3Gi1S"
   },
   "source": [
    "### 1-7. Leaky ReLU 함수에 대한 코드입니다. 코드를 완성해주세요.(5점)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d25442dd",
   "metadata": {
    "id": "fRrCzcx6Ghxa"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.dense = nn.Linear(in_dim, out_dim)  \n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.act(self.dense(x))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce11c28",
   "metadata": {
    "id": "TxZMhY_nn1SW"
   },
   "source": [
    "# 2. BTS맴버들의 이미지를 분류하는 코드를 완성해주세요.\n",
    "- colab환경에서 우측 상단 화살표 -> 런타임 유형변경 -> 하드웨어 가속기 T4 GPU 선택 이후 실행해주세요.\n",
    "\n",
    "- 제공된 bts_classification_simple파일을 구글드라이브에 업로드 한 후 아래코드를 실행해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dab372e0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ufm90za6iUWp",
    "outputId": "a56e531c-15fe-4632-d870-b041cdf54b05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training customized BTS Classfication Data on cpu\n"
     ]
    }
   ],
   "source": [
    "# 문제 x 그냥 실행해 주세요.\n",
    "from __future__ import print_function\n",
    "from torch import nn, optim, cuda\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "batch_size = 4\n",
    "#device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(f'Training customized BTS Classfication Data on {device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774563cb",
   "metadata": {
    "id": "EV5bL9crrPuG"
   },
   "source": [
    "### 2-1. 아래 코드를 완성하세요.(5점)\n",
    "- 본인 구글드라이브의 알맞은 경로 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8dcaceb4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TjMAvJJXihzm",
    "outputId": "b9e59bc6-7ba2-46dc-be2d-fc053309fa1f"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'bts_classification_simple'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m     image_list\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39marray(Image\u001b[39m.\u001b[39mopen(item)))\n\u001b[0;32m     12\u001b[0m     label_folder \u001b[39m=\u001b[39m item\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\n\u001b[1;32m---> 13\u001b[0m     label_list\u001b[39m.\u001b[39mappend(\u001b[39mint\u001b[39;49m(label_folder))\n\u001b[0;32m     15\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoading Training Data... \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m all_image_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy((np\u001b[39m.\u001b[39marray(image_list)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m))\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32))\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'bts_classification_simple'"
     ]
    }
   ],
   "source": [
    "# Assuming `cls_dict` is defined as follows:\n",
    "cls_dict = {0: 'BTS J-Hope', 1: 'BTS Jimin', 2: 'BTS Jin', 3: 'BTS Jungkook', 4: 'BTS RM', 5: 'BTS Suga', 6: 'BTS V'}\n",
    "\n",
    "# Loading Training Data\n",
    "all_image = glob.glob('./bts_classification_simple/train/*/*.jpg')\n",
    "all_image.sort()\n",
    "image_list = []\n",
    "label_list = []\n",
    "\n",
    "for item in all_image:\n",
    "    image_list.append(np.array(Image.open(item)))\n",
    "    label_folder = item.split('/')[-2]\n",
    "    label_list.append(int(label_folder))\n",
    "\n",
    "print(\"Loading Training Data... \")\n",
    "\n",
    "all_image_tensor = torch.from_numpy((np.array(image_list).transpose(0, 3, 1, 2)).astype(np.float32))\n",
    "target_tensor = torch.tensor(np.array(label_list))\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(all_image_tensor, target_tensor)\n",
    "\n",
    "# Loading Test Data\n",
    "test_image = glob.glob('./bts_classification_simple/test/*/*.jpg')\n",
    "test_image.sort()\n",
    "test_image_list = []\n",
    "test_label_list = []\n",
    "\n",
    "for item in test_image:\n",
    "    test_image_list.append(np.array(Image.open(item)))\n",
    "    label_folder = item.split('/')[-2]\n",
    "    test_label_list.append(int(label_folder))\n",
    "\n",
    "print(\"Loading Test Data... \")\n",
    "\n",
    "test_image_tensor = torch.from_numpy((np.array(test_image_list).transpose(0, 3, 1, 2)).astype(np.float32))\n",
    "test_target_tensor = torch.tensor(np.array(test_label_list))\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(test_image_tensor, test_target_tensor)\n",
    "\n",
    "# Data loader\n",
    "batch_size = 32  # Set appropriate batch size\n",
    "train_loader = data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = data.DataLoader(dataset=test_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b07bf9",
   "metadata": {
    "id": "_6B92qsjqjgx"
   },
   "source": [
    "### 2-2. 아래 코드를 완성하세요.(10점)\n",
    "- 활성화함수 : relu\n",
    "- 손실함수 : CrossEntropy\n",
    "- optimizer : SGD\n",
    "- learning rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9609dddd",
   "metadata": {
    "id": "JWG44zugiy70"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.l2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.l3 = nn.Conv2d(32, 32, 3, padding=1)\n",
    "        self.l4 = nn.Conv2d(32, 32, 3, padding=1)\n",
    "        self.l5 = nn.Conv2d(32, 32, 3, padding=1)\n",
    "        self.l6 = nn.Linear(1568, 120)\n",
    "        self.l7 = nn.Linear(120, 10)  # Assuming 10 classes for classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.l1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.l2(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.l3(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.l4(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.l5(x)), 2)\n",
    "        x = x.view(-1, 1568)\n",
    "        x = F.relu(self.l6(x))\n",
    "        return self.l7(x)\n",
    "\n",
    "# Model Initialize\n",
    "model = Net()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer Initialize\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eec7ee",
   "metadata": {
    "id": "6dfvelhpsL8X"
   },
   "source": [
    "### 2-3. 아래 코드를 완성해 주세요(10점)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c50ef8f8",
   "metadata": {
    "id": "E03aGh_ol98Y"
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # optimizer의 기울기를 초기화합니다.\n",
    "        output = model(data)\n",
    "\n",
    "        loss = criterion(output, target)  # 손실을 계산합니다.\n",
    "        loss.backward()  # 역전파 단계를 수행하여 기울기를 계산합니다.\n",
    "        optimizer.step()  # 가중치를 업데이트합니다.\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(img_show=False):\n",
    "    model.eval()  # 모델을 평가 모드로 설정합니다.\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    cnt = 0\n",
    "    with torch.no_grad():  # 테스트 시에는 기울기를 계산하지 않습니다.\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "\n",
    "            # 배치 손실을 더합니다.\n",
    "            test_loss += criterion(output, target).item()\n",
    "\n",
    "            # 예측값의 인덱스를 가져옵니다.\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "            if img_show and cnt < 5:  # 이미지를 보여줄 때\n",
    "                plt.imshow(((data[0].to(\"cpu\")).numpy() * 0.0039216).transpose(1, 2, 0))\n",
    "                plt.show()\n",
    "                print(\"Target: \", cls_dict[int(target.to(\"cpu\").numpy())])\n",
    "                print(\"Pred: \", cls_dict[int(pred.to(\"cpu\").numpy())])\n",
    "\n",
    "            cnt += 1\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'==================\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)}'\n",
    "          f'({100. * correct / len(test_loader.dataset):.0f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29c6bf47",
   "metadata": {
    "id": "59GiSgBHuqZn"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [4, 10]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m11\u001b[39m):\n\u001b[0;32m      4\u001b[0m   epoch_start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m----> 5\u001b[0m   train(epoch)\n\u001b[0;32m      6\u001b[0m   m, s \u001b[39m=\u001b[39m \u001b[39mdivmod\u001b[39m(time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m epoch_start, \u001b[39m60\u001b[39m)\n\u001b[0;32m      7\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTraining time: \u001b[39m\u001b[39m{\u001b[39;00mm\u001b[39m:\u001b[39;00m\u001b[39m.0f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39mm \u001b[39m\u001b[39m{\u001b[39;00ms\u001b[39m:\u001b[39;00m\u001b[39m.0f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39ms\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m      4\u001b[0m data, target \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device), target\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()  \u001b[39m# optimizer의 기울기를 초기화합니다.\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m output \u001b[39m=\u001b[39m model(data)\n\u001b[0;32m      9\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, target)  \u001b[39m# 손실을 계산합니다.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m loss\u001b[39m.\u001b[39mbackward()  \u001b[39m# 역전파 단계를 수행하여 기울기를 계산합니다.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 13\u001b[0m, in \u001b[0;36mNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 13\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmax_pool2d(F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ml1(x)), \u001b[39m2\u001b[39m)\n\u001b[0;32m     14\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmax_pool2d(F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml2(x)), \u001b[39m2\u001b[39m)\n\u001b[0;32m     15\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmax_pool2d(F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml3(x)), \u001b[39m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    457\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [4, 10]"
     ]
    }
   ],
   "source": [
    "# 학습후 매 Epoch당 test Accuracy를 확인해 보세요.\n",
    "since = time.time()\n",
    "for epoch in range(1, 11):\n",
    "  epoch_start = time.time()\n",
    "  train(epoch)\n",
    "  m, s = divmod(time.time() - epoch_start, 60)\n",
    "  print(f'Training time: {m:.0f}m {s:.0f}s')\n",
    "  if epoch == 1:\n",
    "    test(True)\n",
    "\n",
    "  if epoch < 10:\n",
    "    test()\n",
    "  if epoch == 10:\n",
    "    test(True)\n",
    "  m, s = divmod(time.time() - epoch_start, 60)\n",
    "  print(f'Tesing time: {m:.0f}m {s:.0f}s')\n",
    "\n",
    "m, s = divmod(time.time() - since, 60)\n",
    "print(f'Total time : {m:.0f}m {s: .0f}s \\nModel was trained on {device}!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef26e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "3fcbd7240ee8f908d933dc7f71e8c42a1a91163b70ede8dcff5146d4087436c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
