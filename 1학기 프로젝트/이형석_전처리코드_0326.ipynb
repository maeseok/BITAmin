{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1XEiJJlmbDO"
      },
      "source": [
        "# 제주어 데이터프레임화 시키기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AU1vh3Pmg2E"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# JSON 파일 경로\n",
        "file_path = '../../1학기 프로젝트/DZES20000002.json'\n",
        "\n",
        "# 필요한 speaker_id, form, standard_form, dialect_form, isDialect 부분만 추출\n",
        "def load_and_extract_data(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "    print(data)\n",
        "    utterances = data['utterance']\n",
        "    print(data['utterance'])\n",
        "    extracted_data = []\n",
        "\n",
        "    for utterance in utterances:\n",
        "        speaker_id = utterance['speaker_id']\n",
        "        form = utterance['form']\n",
        "        standard_form = utterance['standard_form']\n",
        "        dialect_form = utterance['dialect_form']\n",
        "        isDialect = any(eojeol['isDialect'] for eojeol in utterance['eojeolList'])  # Check if any eojeol is a dialect\n",
        "\n",
        "        utterance_data = {\n",
        "            'speaker_id': speaker_id,\n",
        "            'form': form,\n",
        "            'standard_form': standard_form,\n",
        "            'dialect_form': dialect_form,\n",
        "            'isDialect': isDialect,\n",
        "        }\n",
        "\n",
        "        extracted_data.append(utterance_data)\n",
        "\n",
        "    return extracted_data\n",
        "\n",
        "# 데이터프레임화\n",
        "extracted_data = load_and_extract_data(file_path)\n",
        "df = pd.DataFrame(extracted_data)\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF1fxVj7mkvg"
      },
      "source": [
        "Training 폴더 안에 많은 json 파일들이 있어.\n",
        "\n",
        "폴더 경로만 입력해주면 자동으로 json 파일을 탐색해서\n",
        "아래 코드처럼 데이터프레임화 시켜주는 코드를 작성해줄래??\n",
        "\n",
        "## json 파일 읽어서 데이터프레임화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpX0WnNvmkLv"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Define the function to process JSON files in a directory\n",
        "def process_json_files(directory_path):\n",
        "    # Use glob to find all JSON files in the directory\n",
        "    json_files = glob.glob(os.path.join(directory_path, '*.json'))\n",
        "\n",
        "    # Loop over each file path\n",
        "    for file_path in json_files:\n",
        "        # Use your existing function to load and extract data\n",
        "        extracted_data = load_and_extract_data(file_path)\n",
        "\n",
        "        # Create a DataFrame from the extracted data\n",
        "        df = pd.DataFrame(extracted_data)\n",
        "\n",
        "        # Here you can either return the DataFrame,\n",
        "        # print it out, or perhaps write it to a CSV or another file\n",
        "        print(f\"Processed {file_path}\")\n",
        "        # For example, you could save each as a CSV\n",
        "        df.to_csv(file_path.replace('.json', '.csv'), index=False)\n",
        "\n",
        "        # If you want to combine all DataFrames into one, you could append them to a list\n",
        "        # and concatenate them after the loop\n",
        "\n",
        "    # If combining, return the concatenated DataFrame\n",
        "    # return pd.concat(dfs_list, ignore_index=True)\n",
        "\n",
        "# Path to the Training directory\n",
        "training_dir_path = '../chatbot/Training'  # Replace with your actual directory path\n",
        "\n",
        "# Execute the function\n",
        "process_json_files(training_dir_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diKw3qafm04Y"
      },
      "source": [
        "## csv 파일 읽어서 위 아래로 병합"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIEvTPhum0S5"
      },
      "outputs": [],
      "source": [
        "# Execute the function\n",
        "process_json_files(training_dir_path)\n",
        "\n",
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Replace with your actual directory path\n",
        "training_dir_path = '../chatbot/Training'\n",
        "\n",
        "# Use glob to list all CSV files in the directory\n",
        "csv_files = glob.glob(os.path.join(training_dir_path, '*.csv'))\n",
        "\n",
        "# Initialize an empty list to store DataFrames\n",
        "dataframes = []\n",
        "\n",
        "# Loop through all the files\n",
        "for csv_file in csv_files:\n",
        "    # Read the current CSV into a DataFrame\n",
        "    df = pd.read_csv(csv_file)\n",
        "    # Append the DataFrame to the list\n",
        "    dataframes.append(df)\n",
        "\n",
        "# Concatenate all the DataFrames into a single one\n",
        "merged_df = pd.concat(dataframes, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yn4XBBvhm5nk"
      },
      "outputs": [],
      "source": [
        "# 불용어 제거 - 감정표현\n",
        "merged_df['standard_form'] = merged_df['standard_form'].apply(lambda x: str(x).replace('(', ''))\n",
        "merged_df['standard_form'] = merged_df['standard_form'].apply(lambda x: str(x).replace(')', ''))\n",
        "merged_df['standard_form'] = merged_df['standard_form'].apply(lambda x: str(x).replace('#', ''))\n",
        "merged_df['standard_form'] = merged_df['standard_form'].apply(lambda x: str(x).replace('{laughing}', ''))\n",
        "merged_df['standard_form'] = merged_df['standard_form'].apply(lambda x: str(x).replace('{laughing5}', ''))\n",
        "merged_df['standard_form'] = merged_df['standard_form'].apply(lambda x: str(x).replace('{laughing6}', ''))\n",
        "merged_df['standard_form'] = merged_df['standard_form'].apply(lambda x: str(x).replace('{applauding}', ''))\n",
        "merged_df['standard_form'] = merged_df['standard_form'].apply(lambda x: str(x).replace('{clearing}', ''))\n",
        "merged_df['standard_form'] = merged_df['standard_form'].apply(lambda x: str(x).replace('{singing}', ''))\n",
        "\n",
        "merged_df['dialect_form'] = merged_df['dialect_form'].apply(lambda x: str(x).replace('(', ''))\n",
        "merged_df['dialect_form'] = merged_df['dialect_form'].apply(lambda x: str(x).replace(')', ''))\n",
        "merged_df['dialect_form'] = merged_df['dialect_form'].apply(lambda x: str(x).replace('#', ''))\n",
        "merged_df['dialect_form'] = merged_df['dialect_form'].apply(lambda x: str(x).replace('{laughing}', ''))\n",
        "merged_df['dialect_form'] = merged_df['dialect_form'].apply(lambda x: str(x).replace('{laughing5}', ''))\n",
        "merged_df['dialect_form'] = merged_df['dialect_form'].apply(lambda x: str(x).replace('{laughing6}', ''))\n",
        "merged_df['dialect_form'] = merged_df['dialect_form'].apply(lambda x: str(x).replace('{applauding}', ''))\n",
        "merged_df['dialect_form'] = merged_df['dialect_form'].apply(lambda x: str(x).replace('{clearing}', ''))\n",
        "merged_df['dialect_form'] = merged_df['dialect_form'].apply(lambda x: str(x).replace('{singing}', ''))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cG-Wl9G2m8px"
      },
      "outputs": [],
      "source": [
        "# 불용어 제거 - 이름\n",
        "\n",
        "for i in range(1, 101):\n",
        "    merged_df['standard_form'] = merged_df['standard_form'].apply(lambda x: str(x).replace(f'&name{i}&', ''))\n",
        "    merged_df['dialect_form'] = merged_df['dialect_form'].apply(lambda x: str(x).replace(f'&name{i}&', ''))\n",
        "\n",
        "    merged_df['standard_form'] = merged_df['standard_form'].apply(lambda x: str(x).replace(f'&company-name{i}&', ''))\n",
        "    merged_df['dialect_form'] = merged_df['dialect_form'].apply(lambda x: str(x).replace(f'&company-name{i}&', ''))\n",
        "\n",
        "\n",
        "merged_df['standard_form'] = merged_df['standard_form'].apply(lambda x: str(x).replace('&name&', ''))\n",
        "merged_df['dialect_form'] = merged_df['dialect_form'].apply(lambda x: str(x).replace('&name&', ''))\n",
        "merged_df['standard_form'] = merged_df['standard_form'].apply(lambda x: str(x).replace('&name5 &', ''))\n",
        "merged_df['dialect_form'] = merged_df['dialect_form'].apply(lambda x: str(x).replace('&name5 &', ''))\n",
        "merged_df['standard_form'] = merged_df['standard_form'].apply(lambda x: str(x).replace('&namE5&', ''))\n",
        "merged_df['dialect_form'] = merged_df['dialect_form'].apply(lambda x: str(x).replace('&namE5&', ''))\n",
        "merged_df['standard_form'] = merged_df['standard_form'].apply(lambda x: str(x).replace('&name3$', ''))\n",
        "merged_df['dialect_form'] = merged_df['dialect_form'].apply(lambda x: str(x).replace('&name3$', ''))\n",
        "merged_df['standard_form'] = merged_df['standard_form'].apply(lambda x: str(x).replace('@name@', ''))\n",
        "merged_df['dialect_form'] = merged_df['dialect_form'].apply(lambda x: str(x).replace('@name@', ''))\n",
        "merged_df['standard_form'] = merged_df['standard_form'].apply(lambda x: str(x).replace('@name3', ''))\n",
        "merged_df['dialect_form'] = merged_df['dialect_form'].apply(lambda x: str(x).replace('@name3', ''))\n",
        "merged_df['standard_form'] = merged_df['standard_form'].apply(lambda x: str(x).replace('&name@', ''))\n",
        "merged_df['dialect_form'] = merged_df['dialect_form'].apply(lambda x: str(x).replace('&name@', ''))\n",
        "merged_df['standard_form'] = merged_df['standard_form'].apply(lambda x: str(x).replace('&company-name&', ''))\n",
        "merged_df['dialect_form'] = merged_df['dialect_form'].apply(lambda x: str(x).replace('&company-name&', ''))\n",
        "merged_df['standard_form'] = merged_df['standard_form'].apply(lambda x: str(x).replace('&compny-name2&', ''))\n",
        "merged_df['dialect_form'] = merged_df['dialect_form'].apply(lambda x: str(x).replace('&compny-name2&', ''))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHGef_YanBYo"
      },
      "source": [
        "파이썬 데이터프레임의 standard_form column과 dialect_form column에 대해서 전처리 해줄거야.\n",
        "\n",
        "띄어쓰기를 기준으로 토큰화해서 각 요소들을 살피는데\n",
        "만약 토큰 중에 '/' 이 포함되어 있다면\n",
        "\n",
        "제주어 column에서는 / 앞에 위치한 단어를 고르고\n",
        "표준어 column에서는 / 뒤에 위치한 단어를 고르도록 전처리해주고 싶어.\n",
        "\n",
        "함수에 대한 자세한 설명과 실제 데이터프레임에 적용하는 부분까지 코드 작성 부탁해.\n",
        "\n",
        "<예시_1>\\\n",
        "예시문장 : 이렇게 하다그네/하다가 큰일 나\\\n",
        "dialect_form : 이렇게 하다그네 큰일 나\\\n",
        "standard_form : 이렇게 하다가 큰일 나\n",
        "\n",
        "<예시_2>\\\n",
        "예시문장 : 겅허고/그리고 예초하다보면 돌멩이가 많이 튀어 위험해\\\n",
        "dialect_form : 겅허고 예초하다보면 돌멩이가 많이 튀어 위험해\\\n",
        "standard_form : 그리고 예초하다보면 돌멩이가 많이 튀어 위험해"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZnUjBabmPDN"
      },
      "outputs": [],
      "source": [
        "# Let's define a function to process the standard_form and dialect_form columns as described\n",
        "def preprocess_forms(row):\n",
        "    # Split the sentence into tokens based on whitespace\n",
        "    dialect_tokens = row['dialect_form'].split()\n",
        "    standard_tokens = row['standard_form'].split()\n",
        "\n",
        "    # For tokens that contain a '/', choose the part before '/' for dialect_form\n",
        "    # and the part after '/' for standard_form\n",
        "    row['dialect_form'] = ' '.join(token.split('/')[0] if '/' in token else token for token in dialect_tokens)\n",
        "    row['standard_form'] = ' '.join(token.split('/')[1] if '/' in token else token for token in standard_tokens)\n",
        "\n",
        "    return row\n",
        "\n",
        "# Apply the preprocess function to each row in the DataFrame\n",
        "processed_df = merged_df.apply(preprocess_forms, axis=1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "e726dfcfd2aa7c89c8dbc4c560c1e1f3c16eb0fa359096469343a30c0228cb32"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
