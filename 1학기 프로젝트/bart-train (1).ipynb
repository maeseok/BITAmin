{"cells":[{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-05-25T08:47:29.528837Z","iopub.status.busy":"2024-05-25T08:47:29.528157Z","iopub.status.idle":"2024-05-25T08:47:29.536587Z","shell.execute_reply":"2024-05-25T08:47:29.535629Z","shell.execute_reply.started":"2024-05-25T08:47:29.528801Z"},"id":"im1z8EKArFf6","outputId":"ced2ae00-21ed-454c-d8c0-051e88ae352d","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]}],"source":["import transformers\n","import torch\n","import pandas as pd\n","import numpy as np\n","import os\n","from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW, BitsAndBytesConfig\n","from tqdm.notebook import tqdm\n","\n","# Check for CUDA availability\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install transformers\n","!pip install torch\n","!pip install tpdm"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6558e8b9","outputId":"3e498121-b4fe-4efc-d5c3-1e742e4e3910","trusted":true},"outputs":[],"source":["!pip install torch torchvision torchaudio\n","!pip install datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install wandb"]},{"cell_type":"markdown","metadata":{"id":"d2b43e3e"},"source":["# 1. 데이터 수집 및 준비"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"execution":{"iopub.execute_input":"2024-05-25T08:50:16.744504Z","iopub.status.busy":"2024-05-25T08:50:16.743485Z","iopub.status.idle":"2024-05-25T08:50:25.848093Z","shell.execute_reply":"2024-05-25T08:50:25.847202Z","shell.execute_reply.started":"2024-05-25T08:50:16.744463Z"},"id":"1e308787","outputId":"63cb2aef-4247-4558-b7ef-c5333b50bb1a","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_34/2456427794.py:1: DtypeWarning: Columns (0,2,4,5,6,7,8,9,12,13) have mixed types. Specify dtype option on import or set low_memory=False.\n","  train_data = pd.read_csv('/kaggle/input/train-data2/0523_942292row.csv').iloc[:,1:]\n"]},{"name":"stdout","output_type":"stream","text":["(942292, 17)\n"]}],"source":["train_data = pd.read_csv('/kaggle/input/train-data2/0523_942292row.csv').iloc[:,1:]\n","train_data = train_data.loc[train_data.standard_form.notnull()]"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T08:50:54.783993Z","iopub.status.busy":"2024-05-25T08:50:54.783115Z","iopub.status.idle":"2024-05-25T08:50:55.094487Z","shell.execute_reply":"2024-05-25T08:50:55.093467Z","shell.execute_reply.started":"2024-05-25T08:50:54.783955Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(941792, 17)\n"]}],"source":["eval_data = train_data.sample(n=500)\n","train_data = train_data.drop(eval_data.index, errors='ignore')\n","    "]},{"cell_type":"markdown","metadata":{"id":"b466abbc"},"source":["# 2. 모델 선택 및 수정\n","- 모델은 KoBART 모델을 사용하고 싶음\n","- 한국어로 학습되어 있고 무게가 그렇게 무겁지 않아 사용하기에 적절하다고 판단\n","- URL : gogamza/kobart-base-v2"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-05-25T08:51:11.231677Z","iopub.status.busy":"2024-05-25T08:51:11.231291Z","iopub.status.idle":"2024-05-25T08:51:12.002723Z","shell.execute_reply":"2024-05-25T08:51:12.001899Z","shell.execute_reply.started":"2024-05-25T08:51:11.231644Z"},"id":"d05f323a","outputId":"0bcc119a-7d27-4448-ea8e-f404e318cd8d","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"]}],"source":["from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration\n","\n","# 모델과 토크나이저 로드\n","checkpoint_path = '/kaggle/input/model-train-ver2'\n","tokenizer = PreTrainedTokenizerFast.from_pretrained(checkpoint_path)\n","model = BartForConditionalGeneration.from_pretrained(checkpoint_path)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T08:51:15.840774Z","iopub.status.busy":"2024-05-25T08:51:15.840049Z","iopub.status.idle":"2024-05-25T08:51:16.537768Z","shell.execute_reply":"2024-05-25T08:51:16.536689Z","shell.execute_reply.started":"2024-05-25T08:51:15.840740Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["          year topic  speaker_id  age sex    occupation birthplace education  \\\n","0       2020.0    가족         1.0  50대  여성  전문가 및 관련 종사자         제주        대졸   \n","1       2020.0    가족         1.0  50대  여성  전문가 및 관련 종사자         제주        대졸   \n","2       2020.0    가족         1.0  50대  여성  전문가 및 관련 종사자         제주        대졸   \n","3       2020.0    가족         1.0  50대  여성  전문가 및 관련 종사자         제주        대졸   \n","4       2020.0    가족         1.0  50대  여성  전문가 및 관련 종사자         제주        대졸   \n","...        ...   ...         ...  ...  ..           ...        ...       ...   \n","832461  2021.0    게임         1.0  30대  여성  전문가 및 관련 종사자         제주    대학원 이상   \n","832462  2021.0    게임         3.0  30대  여성        사무 종사자         제주        대졸   \n","832463  2021.0    게임         3.0  30대  여성        사무 종사자         제주        대졸   \n","832464  2021.0    게임         2.0  30대  여성      무직/취업준비생         제주    대학원 이상   \n","832465  2021.0    게임         3.0  30대  여성        사무 종사자         제주        대졸   \n","\n","                                                     form  \\\n","0                                       어 그거 (튀겨그네)/(튀겨서)   \n","1                               돌멩이 해 가지고 실명된사람 (이서)/(있어)   \n","2                 (거난)/(그러니까) 제주도는 워낙 또 돌멩이가 (많아그네)/(많아서)   \n","3                                           (겅하니깐)/(그러니깐)   \n","4                       데리고 가고 (싶어그네)/(싶어서) 할아버지가(이)/(#이)   \n","...                                                   ...   \n","832461          두 명 이 인용 보드게임 이런 것도 (잘도)/(매우) 잘 팔린(댄)/(대)   \n","832462  나 집에 할리갈리 (이서)/(있어) 루미큐브도 (이서)/(있어) 할리갈리는 보드게임...   \n","832463                 색깔 반전 시키는 것도 (있댄)/(있다고) 하고(이)/(#이)   \n","832464   @이름3이 @이름3이가 그거 (아니)/(아니야) (쬐그만한)/(작은) 거 가지고 있던데   \n","832465                        어 맞아 나 좀 (쬐끄만한)/(작은)(디)/(데)   \n","\n","                                 standard_form  \\\n","0                                     어 그거 튀겨서   \n","1                           돌멩이 해 가지고 실명된사람 있어   \n","2                      그러니까 제주도는 워낙 또 돌멩이가 많아서   \n","3                                         그러니깐   \n","4                            데리고 가고 싶어서 할아버지가이   \n","...                                        ...   \n","832461            두 명 이 인용 보드게임 이런 것도 매우 잘 팔린댄   \n","832462  나 집에 할리갈리 있어 루미큐브도 있어 할리갈리는 보드게임이 아니야    \n","832463                    색깔 반전 시키는 것도 있다고 하고이   \n","832464                이 이가 그거 아니야 작은 거 가지고 있던데   \n","832465                            어 맞아 나 좀 작은데   \n","\n","                                   dialect_form isDialect  \\\n","0                                     어 그거 튀겨그네        제주   \n","1                            돌멩이 해 가지고 실명된사람 이서        제주   \n","2                        거난 제주도는 워낙 또 돌멩이가 많아그네        제주   \n","3                                          겅하니깐        제주   \n","4                            데리고 가고 싶어그네 할아버지가이        제주   \n","...                                         ...       ...   \n","832461            두 명 이 인용 보드겡이임 이런 것도 잘도 잘 팔린댄        제주   \n","832462  나 집에 할리갈리 이서 루미큐브도 이서 할리갈리는 보드겡이임이 아니야         제주   \n","832463                      색깔 반전 시키는 것도 있댄 하고이        제주   \n","832464                이 이가 그거 아니 쬐그만한 거 가지고 있던데        제주   \n","832465                           어 맞아 나 좀 쬐끄만한디        제주   \n","\n","                        extracted  standard_len  dialect_len  linear_kernel  \\\n","0                    (튀겨그네)/(튀겨서)           8.0          9.0       0.383816   \n","1                       (이서)/(있어)          18.0         18.0       0.852266   \n","2       (거난)/(그러니까), (많아그네)/(많아서)          23.0         22.0       0.770262   \n","3                   (겅하니깐)/(그러니깐)           4.0          4.0       0.000000   \n","4          (싶어그네)/(싶어서), (이)/(#이)          17.0         18.0       0.886922   \n","...                           ...           ...          ...            ...   \n","832461         (잘도)/(매우), (댄)/(대)          28.0         28.0       0.774979   \n","832462       (이서)/(있어), (이서)/(있어)          38.0         38.0       0.654706   \n","832463       (있댄)/(있다고), (이)/(#이)          20.0         19.0       0.830916   \n","832464    (아니)/(아니야), (쬐그만한)/(작은)          24.0         25.0       0.581616   \n","832465       (쬐끄만한)/(작은), (디)/(데)          12.0         14.0       0.478422   \n","\n","        jaccard_similarity  \n","0                 0.500000  \n","1                 0.666667  \n","2                 0.500000  \n","3                 0.000000  \n","4                 0.600000  \n","...                    ...  \n","832461            0.666667  \n","832462            0.600000  \n","832463            0.714286  \n","832464            0.600000  \n","832465            0.666667  \n","\n","[832019 rows x 17 columns]\n"]}],"source":["## 제주어 표준어 구분\n","## 이거 사실 안해도 되는데 True False 보다는 string 이 나을거 같아서 그냥 변환\n","train_data.isDialect = train_data.isDialect.apply(lambda x : '제주' if x == True else '표준')\n","\n","\n","## 제주 발화만 남기기\n","## 사실 isDialect가 표준인 경우에는 두 문장이 모두 표준어이기 때문에 source, target 문장이 같아서 의미가 없다고 생각해 제거\n","train_data = train_data.loc[train_data.isDialect == '제주']\n","#train_data = train_data.sample(n=600000)\n","print(train_data)"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T08:51:19.228580Z","iopub.status.busy":"2024-05-25T08:51:19.227802Z","iopub.status.idle":"2024-05-25T09:05:06.468234Z","shell.execute_reply":"2024-05-25T09:05:06.467174Z","shell.execute_reply.started":"2024-05-25T08:51:19.228542Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3892: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"]}],"source":["## 제주어 토큰, 표준어 토큰 정의하기\n","jeju_token = \"[제주]\"\n","standard_token = \"[표준]\"\n","\n","## 양방향 데이터 리스트 생성\n","bidirectional_data = []\n","\n","for dialect, standard in zip(train_data['dialect_form'], train_data['standard_form']):\n","    ## 토큰이 [제주] 일 경우 제주어 -> 표준어\n","    bidirectional_data.append({\n","        \"source\": jeju_token + \" \" + dialect,\n","        \"target\": standard\n","    })\n","    ## 토큰이 [표준] 일 경우 표준어 -> 제주어\n","    bidirectional_data.append({\n","        \"source\": standard_token + \" \" + standard,\n","        \"target\": dialect\n","    })\n","\n","## 데이터 토크나이징\n","tokenized_data = []\n","for item in bidirectional_data:\n","    source_encodings = tokenizer(item['source'], max_length=64, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n","    with tokenizer.as_target_tokenizer():\n","        target_encodings = tokenizer(item['target'], max_length=64, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n","    tokenized_data.append({\n","        \"input_ids\": source_encodings[\"input_ids\"],\n","        \"attention_mask\": source_encodings[\"attention_mask\"],\n","        \"labels\": target_encodings[\"input_ids\"]\n","    })\n","## 데이터가 어떻게 토큰화되었는지 한번 확인해보슈\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T09:05:21.984598Z","iopub.status.busy":"2024-05-25T09:05:21.983700Z"},"trusted":true},"outputs":[],"source":["from datasets import Dataset, DatasetDict\n","import pandas as pd\n","\n","formatted_data_df = pd.DataFrame([{\n","    \"input_ids\": np.array(fd[\"input_ids\"].numpy().tolist()[0], dtype=np.uint16),\n","    \"attention_mask\": np.array(fd[\"attention_mask\"].numpy().tolist()[0], dtype=np.uint8),\n","    \"labels\": np.array(fd[\"labels\"].numpy().tolist()[0], dtype=np.uint16)\n","} for fd in tokenized_data])\n","\n","## 변환된 데이터를 DataFrame으로 변환\n","#formatted_data_df = pd.DataFrame([{\n","#    \"input_ids\": fd[\"input_ids\"].numpy().tolist()[0],  ## Tensor를 리스트로 변환\n","#    \"attention_mask\": fd[\"attention_mask\"].numpy().tolist()[0],  ## Tensor를 리스트로 변환\n","#    \"labels\": fd[\"labels\"].numpy().tolist()[0]  ## Tensor를 리스트로 변환\n","#} for fd in tokenized_data])\n","\n","## 데이터를 Dataset 형식으로 변환\n","train_dataset = Dataset.from_pandas(formatted_data_df)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T08:43:05.129665Z","iopub.status.busy":"2024-05-25T08:43:05.128836Z","iopub.status.idle":"2024-05-25T08:43:05.768502Z","shell.execute_reply":"2024-05-25T08:43:05.767553Z","shell.execute_reply.started":"2024-05-25T08:43:05.129633Z"},"trusted":true},"outputs":[],"source":["## 학습 데이터셋을 학습 및 평가용으로 분리 (예: 95% 학습, 5% 평가)\n","train_test_split = train_dataset.train_test_split(test_size=0.05)\n","dataset_dict = DatasetDict({\n","    'train': train_test_split['train'],\n","    'test': train_test_split['test']\n","})"]},{"cell_type":"markdown","metadata":{"id":"10d45c8c"},"source":["# 3. 모델 학습 및 fine-tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cb8967f3","outputId":"5bcd46ea-0dd5-49f9-c8a5-038cf0d6dad3","trusted":true},"outputs":[],"source":["#! pip install -U accelerate\n","#! pip install -U transformers\n","\n","import accelerate\n","import transformers\n","\n","transformers.__version__, accelerate.__version__"]},{"cell_type":"markdown","metadata":{},"source":["## 3-1. 기존 모델에 추가 학습하는 경우"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4f26f313","trusted":true},"outputs":[],"source":["#check120000 이후 \n","from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n","\n","# Define training arguments\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir='/kaggle/working/chatbot',          # output directory for model checkpoints    \n","    learning_rate=2e-5,   \n","    num_train_epochs=4,              # total number of training epochs\n","    predict_with_generate=True,   \n","    per_device_train_batch_size=16,   # batch size per device during training\n","    warmup_steps=15000,                # number of warmup steps for learning rate scheduler\n","    weight_decay=0.01,               # strength of weight decay\n","    logging_dir='./logs',            # directory for storing logs\n","    logging_steps=1000,\n","    save_total_limit=4,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",           # Save checkpoint every specified number of steps\n","    resume_from_checkpoint=checkpoint_path, #이전에 저장된 체크포인트부터 학습 재시작\n","    load_best_model_at_end=True,     # 학습이 종료될 때 최적 모델을 로드\n","    metric_for_best_model=\"eval_loss\",  # 최적 모델을 결정하는 메트릭\n",")\n","\n","import wandb\n","from transformers import AdamW,get_linear_schedule_with_warmup,EarlyStoppingCallback\n","\n","optimizer = AdamW(model.parameters(), lr=training_args.learning_rate)\n","\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=training_args.warmup_steps, \n","    num_training_steps=197608  # 총 훈련 스텝 수\n",")\n","# API 키를 직접 입력\n","wandb.login(key=\"\")\n","\n","trainer = Seq2SeqTrainer(\n","    model=model,                         ## 학습할 모델\n","    args=training_args,                  ## 학습 설정\n","    train_dataset=dataset_dict['train'], ## 학습 데이터셋\n","    eval_dataset=dataset_dict['test'],   ## 평가 데이터셋\n","    tokenizer=tokenizer,\n","    optimizers=(optimizer,scheduler), #AdamW\n","    callbacks=[EarlyStoppingCallback(  # 조기 종료 콜백\n","        early_stopping_patience=1,\n","        early_stopping_threshold=None,  # 옵션: 최소 향상 정도\n","    )]\n",")\n","\n","## 학습 시작\n","trainer.train()\n","\n","## 학습 종료\n","wandb.finish()"]},{"cell_type":"markdown","metadata":{},"source":["## 3-2. 모델에 새로 학습하는 경우"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#새로운 모델 학습 버전\n","from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n","#처음부터 다시 모델 학습 용도\n","# Define training arguments\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir='/kaggle/working/chatbot',          # output directory for model checkpoints    \n","    learning_rate=2e-5,   \n","    predict_with_generate=True,   \n","    per_device_train_batch_size=16,   # batch size per device during training\n","    warmup_steps=15000,                # number of warmup steps for learning rate scheduler\n","    weight_decay=0.01,               # strength of weight decay\n","    logging_dir='./logs',            # directory for storing logs\n","    logging_steps=1000,\n","    num_train_epochs=4,              # total number of training epochs\n","    save_total_limit=4,\n","    save_strategy=\"epoch\"           # Save checkpoint every specified number of steps\n",")\n","\n","import wandb\n","from transformers import AdamW,get_linear_schedule_with_warmup\n","\n","# API 키를 직접 입력\n","wandb.login(key=\"aff67fd137854b3632153f22c1ff7d40506aa349\")\n","optimizer = AdamW(model.parameters(), lr=training_args.learning_rate)\n","\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=training_args.warmup_steps, \n","    num_training_steps=395424  # 총 훈련 스텝 수\n",")\n","\n","trainer = Seq2SeqTrainer(\n","    model=model,                         ## 학습할 모델\n","    args=training_args,                  ## 학습 설정\n","    train_dataset=dataset_dict['train'], ## 학습 데이터셋\n","    eval_dataset=dataset_dict['test'],   ## 평가 데이터셋\n","    tokenizer=tokenizer,\n","    optimizers=(optimizer,scheduler) #AdamW\n",")\n","\n","## 학습 시작\n","trainer.train()\n","\n","## 학습 종료\n","wandb.finish()"]},{"cell_type":"markdown","metadata":{},"source":["# 4. 모델 평가"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import transformers\n","import torch\n","import pandas as pd\n","import numpy as np\n","import os\n","from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW, BitsAndBytesConfig\n","from tqdm.notebook import tqdm\n","\n","# Check for CUDA availability\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast,AutoModelForSeq2SeqLM\n","from transformers import AutoModelForMaskedLM\n","# 모델과 토크나이저의 경로\n","model_path = '/kaggle/input/model-0524-12000' # 본인 모델 경로 설정\n","\n","# 모델과 토크나이저 로드\n","model = BartForConditionalGeneration.from_pretrained(model_path)\n","tokenizer = PreTrainedTokenizerFast.from_pretrained(model_path)\n","\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["## 데이터 프레임 로드 및 준비\n","#df_val = pd.read_csv(\"/kaggle/input/bleu-ver1/.csv\")\n","\n","df_val = eval_data #맨 처음 단계에서 500개 미리 추출 후 train_data에서 삭제함 \n","\n","#df_val = df_val.head(500)\n","#df_val=df_val.sample(n=500)\n","## 번역 Task의 문장만 남기기 & 결측치 제거\n","df_val = df_val.loc[df_val.isDialect == True]\n","df_val = df_val.dropna(subset=['standard_form', 'dialect_form'])\n","\n","## 불용어 제거\n","df_val.form_샵 = df_val.form.apply(lambda x : 1 if '#' in x else 0)\n","df_val = df_val.loc[df_val.form_샵 == 0]\n","df_val.form_앤드 = df_val.form.apply(lambda x : 1 if '&' in x else 0)\n","df_val = df_val.loc[df_val.form_앤드 == 0]\n","\n","## 문장을 4 ~ 64로 조정\n","df_val['form_len'] = df_val.standard_form.apply(lambda x : len(str(x)))\n","df_val = df_val.loc[(3 < df_val['form_len']) & (df_val['form_len'] <= 64)]\n","\n","df_val"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from nltk.translate.bleu_score import corpus_bleu\n","from transformers import PreTrainedTokenizerFast\n","from transformers import pipeline\n","\n","# 번역을 위한 파이프라인 생성\n","translation_pipeline = pipeline(\n","    \"translation_xx_to_yy\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    device=0,\n","    max_length = 64\n",")\n","\n","jeju_token = \"[제주]\"\n","standard_token = \"[표준]\"\n","\n","# 모든 번역을 저장할 리스트\n","dialect_targets = []\n","standard_targets = []\n","dialect_predictions = []\n","standard_predictions = []\n","\n","for index, row in df_val.iterrows():\n","    dialect_form = row['dialect_form']\n","    standard_form = row['standard_form']\n","\n","    ## 제주어 -> 표준어 번역 수행\n","    predicted_sentence_dialect = translation_pipeline(jeju_token + \" \" + dialect_form)[0]['translation_text']\n","\n","    ## 표준어 -> 제주어 번역 수행\n","    predicted_sentence_standard = translation_pipeline(standard_token + \" \" + standard_form)[0]['translation_text']\n","\n","    # 토크나이저를 사용하여 문장을 토큰화\n","    dialect_form_tokens = tokenizer.tokenize(dialect_form)\n","    standard_form_tokens = tokenizer.tokenize(standard_form)\n","    predicted_tokens_dialect = tokenizer.tokenize(predicted_sentence_dialect)\n","    predicted_tokens_standard = tokenizer.tokenize(predicted_sentence_standard)\n","\n","    # 토큰화된 문장을 리스트에 추가\n","    dialect_targets.append([dialect_form_tokens])  # 참조는 리스트의 리스트가 되어야 함\n","    standard_targets.append([standard_form_tokens])  # 참조는 리스트의 리스트가 되어야 함\n","    dialect_predictions.append(predicted_tokens_dialect)\n","    standard_predictions.append(predicted_tokens_standard)\n","\n","# corpus_bleu 함수를 사용한 BLEU 점수 계산\n","from_jeju_to_standard_bleu_score = corpus_bleu(standard_targets, dialect_predictions)\n","from_standard_to_jeju_bleu_score = corpus_bleu(dialect_targets, standard_predictions)\n","print(f\"제주어 -> 표준어 번역 BLEU Score : {from_jeju_to_standard_bleu_score}\")\n","print(f\"표준어 -> 제주어 번역 BLEU Score : {from_standard_to_jeju_bleu_score}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["input=df_val['standard_form'].sample(n=3)\n","input2=df_val['dialect_form'].sample(n=3)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from transformers import pipeline\n","\n","# 번역을 위한 파이프라인 생성\n","translation_pipeline = pipeline(\n","    \"translation_xx_to_yy\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    device=0\n",")\n","\n","jeju_token = \"[제주]\"\n","standard_token = \"[표준]\"\n","\n","# 특정 문장 번역 예시\n","for input_sentence in input:\n","    translated_sentence = translation_pipeline(standard_token + \" \" + input_sentence)[0]['translation_text']\n","    print('input_sentence :', input_sentence)\n","    print(\"표준어 -> 제주어:\", translated_sentence)\n","    print()\n","\n","for input_sentence in input2:\n","    translated_sentence = translation_pipeline(jeju_token + \" \" + input_sentence)[0]['translation_text']\n","    print('input_sentence :', input_sentence)\n","    print(\"제주어 -> 표준어:\", translated_sentence)\n","    print()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5007223,"sourceId":8412820,"sourceType":"datasetVersion"},{"datasetId":5009286,"sourceId":8415686,"sourceType":"datasetVersion"},{"datasetId":5029683,"sourceId":8442174,"sourceType":"datasetVersion"},{"datasetId":5033426,"sourceId":8447085,"sourceType":"datasetVersion"},{"datasetId":5036632,"sourceId":8451415,"sourceType":"datasetVersion"},{"datasetId":5054894,"sourceId":8476146,"sourceType":"datasetVersion"},{"datasetId":5055616,"sourceId":8477087,"sourceType":"datasetVersion"},{"datasetId":5071239,"sourceId":8498359,"sourceType":"datasetVersion"},{"datasetId":5076625,"sourceId":8505376,"sourceType":"datasetVersion"},{"datasetId":5080686,"sourceId":8511276,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
