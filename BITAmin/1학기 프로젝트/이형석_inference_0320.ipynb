{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNUYoSPGfKmL"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW, BitsAndBytesConfig\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Check for CUDA availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
        "\n",
        "# 모델과 토크나이저의 경로\n",
        "model_path = '/home/work/chatbot/checkpoints/checkpoint-90000' # 본인 모델 경로 설정\n",
        "\n",
        "# 모델과 토크나이저 로드\n",
        "model = BartForConditionalGeneration.from_pretrained(model_path)\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"gogamza/kobart-base-v2\")\n",
        "\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 추론할 입력 텍스트\n",
        "input_text = \"안녕하세요 오늘따라 날씨가 참 춥네요 저녁은 드셨나요?\"\n",
        "\n",
        "# 입력 텍스트를 토크나이징\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=64, truncation=True, padding=\"max_length\", return_token_type_ids=False) # max_length를 늘리고 줄이고 하면서 확인하기\n",
        "\n",
        "# 토크나이징된 입력을 GPU로 이동 (사용 가능한 경우)\n",
        "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "# 모델을 사용하여 추론 수행\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(**inputs, max_length=64)\n",
        "\n",
        "# 예측 결과 디코딩\n",
        "predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"입력 텍스트:\", input_text)\n",
        "print(\"예측 결과:\", predicted_text)"
      ],
      "metadata": {
        "id": "nSgb4Pt_fPr6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}