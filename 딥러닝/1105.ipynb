{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from tensorflow import keras\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # 28*28 - 0~255로 진행 - keras 데이터로 (sklearn 데이터 x)\n",
    "    (X_train_full, y_train_full), (\n",
    "    X_test, y_test) = tf.keras.datasets.mnist.load_data()  # 훈련 세트는 60,000개의 흑백 이미지입니다. 각 이미지의 크기는 28x28 픽셀입니다:\n",
    "    \n",
    "    # 기존 데이터는 60000:10000 -> 80:20 비율 위해 40000:10000으로 설정\n",
    "    X_train_full=X_train_full[:40000]\n",
    "    y_train_full=y_train_full[:40000]\n",
    "    \n",
    "    return X_train_full, y_train_full, X_test, y_test\n",
    "\n",
    "\n",
    "def data_normalization(X_train_full, y_train_full, X_test, divide):\n",
    "\n",
    "    # 정규화 여부 divide로 결정\n",
    "    X_valid, X_train = X_train_full[:5000] / divide, X_train_full[5000:] / divide\n",
    "    y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "    X_test = X_test / divide\n",
    "\n",
    "    return X_valid, X_train, y_valid, y_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makemodel(X_train, y_train, X_valid, y_valid, weight_init, Dropout=False, epoch=10):\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "    model.add(keras.layers.Dense(300, kernel_initializer=weight_init, activation=\"relu\"))\n",
    "    if Dropout:\n",
    "        model.add(tf.keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer=weight_init, activation=\"relu\"))\n",
    "    if Dropout:\n",
    "        model.add(tf.keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.Dense(10, kernel_initializer=weight_init, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer=\"sgd\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    # 시간 측정\n",
    "    tb_hist = keras.callbacks.TensorBoard(log_dir='./graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "    start = time.time()\n",
    "    history = model.fit(X_train, y_train, epochs=epoch,\n",
    "                        validation_data=(X_valid, y_valid), callbacks=[tb_hist])\n",
    "    print(\"time :\", time.time() - start)\n",
    "    return model, history\n",
    "\n",
    "\n",
    "def evalmodel(model, history, X_test, y_test):\n",
    "    model.evaluate(X_test, y_test)\n",
    "\n",
    "    X_new = X_test[:3]\n",
    "    y_proba = model.predict(X_new)\n",
    "    y_proba.round(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 가중치 초기화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random 가중치 초기화\n",
      "\n",
      "Epoch 1/10\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 1.2923 - accuracy: 0.6854 - val_loss: 0.5139 - val_accuracy: 0.8718\n",
      "Epoch 2/10\n",
      "1094/1094 [==============================] - 4s 3ms/step - loss: 0.4319 - accuracy: 0.8814 - val_loss: 0.3453 - val_accuracy: 0.9050\n",
      "Epoch 3/10\n",
      "1094/1094 [==============================] - 4s 3ms/step - loss: 0.3399 - accuracy: 0.9031 - val_loss: 0.2923 - val_accuracy: 0.9176\n",
      "Epoch 4/10\n",
      "1094/1094 [==============================] - 4s 3ms/step - loss: 0.2961 - accuracy: 0.9144 - val_loss: 0.2641 - val_accuracy: 0.9270\n",
      "Epoch 5/10\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 0.2659 - accuracy: 0.9236 - val_loss: 0.2376 - val_accuracy: 0.9330\n",
      "Epoch 6/10\n",
      "1094/1094 [==============================] - 4s 3ms/step - loss: 0.2408 - accuracy: 0.9309 - val_loss: 0.2188 - val_accuracy: 0.9414\n",
      "Epoch 7/10\n",
      "1094/1094 [==============================] - 4s 3ms/step - loss: 0.2194 - accuracy: 0.9377 - val_loss: 0.2020 - val_accuracy: 0.9442\n",
      "Epoch 8/10\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 0.2012 - accuracy: 0.9427 - val_loss: 0.1880 - val_accuracy: 0.9498\n",
      "Epoch 9/10\n",
      "1094/1094 [==============================] - 4s 3ms/step - loss: 0.1855 - accuracy: 0.9467 - val_loss: 0.1776 - val_accuracy: 0.9506\n",
      "Epoch 10/10\n",
      "1094/1094 [==============================] - 4s 3ms/step - loss: 0.1715 - accuracy: 0.9514 - val_loss: 0.1681 - val_accuracy: 0.9534\n",
      "time : 37.74276876449585\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1734 - accuracy: 0.9499\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "\n",
      " Xavier 가중치 초기화\n",
      "\n",
      "Epoch 1/10\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 0.7625 - accuracy: 0.8073 - val_loss: 0.3602 - val_accuracy: 0.9006\n",
      "Epoch 2/10\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 0.3410 - accuracy: 0.9045 - val_loss: 0.2837 - val_accuracy: 0.9230\n",
      "Epoch 3/10\n",
      "1094/1094 [==============================] - 4s 3ms/step - loss: 0.2815 - accuracy: 0.9212 - val_loss: 0.2445 - val_accuracy: 0.9310\n",
      "Epoch 4/10\n",
      "1094/1094 [==============================] - 4s 3ms/step - loss: 0.2454 - accuracy: 0.9311 - val_loss: 0.2175 - val_accuracy: 0.9400\n",
      "Epoch 5/10\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 0.2180 - accuracy: 0.9389 - val_loss: 0.1996 - val_accuracy: 0.9448\n",
      "Epoch 6/10\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 0.1964 - accuracy: 0.9444 - val_loss: 0.1859 - val_accuracy: 0.9504\n",
      "Epoch 7/10\n",
      "1094/1094 [==============================] - 4s 3ms/step - loss: 0.1777 - accuracy: 0.9502 - val_loss: 0.1722 - val_accuracy: 0.9532\n",
      "Epoch 8/10\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 0.1625 - accuracy: 0.9538 - val_loss: 0.1613 - val_accuracy: 0.9534\n",
      "Epoch 9/10\n",
      "1094/1094 [==============================] - 4s 3ms/step - loss: 0.1495 - accuracy: 0.9579 - val_loss: 0.1505 - val_accuracy: 0.9596\n",
      "Epoch 10/10\n",
      "1094/1094 [==============================] - 4s 3ms/step - loss: 0.1381 - accuracy: 0.9611 - val_loss: 0.1457 - val_accuracy: 0.9598\n",
      "time : 38.91745972633362\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1517 - accuracy: 0.9566\n",
      "1/1 [==============================] - 0s 58ms/step\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # train:test = 80:20\n",
    "    X_train_full, y_train_full, X_test, y_test = load_data()\n",
    "    X_valid, X_train, y_valid, y_train, x_test = data_normalization(X_train_full, y_train_full, X_test, 255.)\n",
    "    \n",
    "    print('Random 가중치 초기화\\n')\n",
    "    model_Random, history_Random = makemodel(X_train, y_train, X_valid, y_valid, 'RandomNormal')\n",
    "    evalmodel(model_Random, history_Random, x_test, y_test)\n",
    "    \n",
    "    print('\\n Xavier 가중치 초기화\\n')\n",
    "    model_Xavier, history_Xavier = makemodel(X_train, y_train, X_valid, y_valid, 'glorot_uniform')\n",
    "    evalmodel(model_Xavier, history_Xavier, x_test, y_test)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 결과 비교  \n",
    " Random과 Xavier 가중치 초기화 방식을 모두 진행했을 때 Xavier를 사용한 경우가  \n",
    " train, val, test 모든 데이터와 전체 학습 기간에 대해 더 낮은 loss와 높은 accuracy를 가진다.  \n",
    " 시간의 경우는 Random보다는 Xavier 방식이 조금 더 걸리는 것을 알 수 있다.  \n",
    " 정리하면, MNIST 데이터셋에 대해 Xavier 가중치 초기화 방식이 학습에 더 효과적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 정규화 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0~255 Ver\n",
      "\n",
      "Epoch 1/10\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 33310.0664 - accuracy: 0.1129 - val_loss: 2.6032 - val_accuracy: 0.1126\n",
      "Epoch 2/10\n",
      "1094/1094 [==============================] - 4s 3ms/step - loss: 2.3146 - accuracy: 0.1143 - val_loss: 2.3712 - val_accuracy: 0.1126\n",
      "Epoch 3/10\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3712 - val_accuracy: 0.1126\n",
      "Epoch 4/10\n",
      "1094/1094 [==============================] - 4s 3ms/step - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3711 - val_accuracy: 0.1126\n",
      "Epoch 5/10\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3710 - val_accuracy: 0.1126\n",
      "Epoch 6/10\n",
      "1094/1094 [==============================] - 4s 3ms/step - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3711 - val_accuracy: 0.1126\n",
      "Epoch 7/10\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3712 - val_accuracy: 0.1126\n",
      "Epoch 8/10\n",
      "1094/1094 [==============================] - 4s 3ms/step - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3712 - val_accuracy: 0.1126\n",
      "Epoch 9/10\n",
      "1094/1094 [==============================] - 3s 3ms/step - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3711 - val_accuracy: 0.1126\n",
      "Epoch 10/10\n",
      "1094/1094 [==============================] - 4s 3ms/step - loss: 2.3011 - accuracy: 0.1143 - val_loss: 2.3713 - val_accuracy: 0.1126\n",
      "time : 36.85619783401489\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.3443 - accuracy: 0.1135\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "\n",
      " 0~1 정규화 Ver\n",
      "\n",
      "Epoch 1/10\n",
      "1094/1094 [==============================] - 4s 3ms/step - loss: 0.7511 - accuracy: 0.8115 - val_loss: 0.3560 - val_accuracy: 0.9042\n",
      "Epoch 2/10\n",
      "1094/1094 [==============================] - 4s 3ms/step - loss: 0.3316 - accuracy: 0.9072 - val_loss: 0.2820 - val_accuracy: 0.9224\n",
      "Epoch 3/10\n",
      "1094/1094 [==============================] - 4s 3ms/step - loss: 0.2746 - accuracy: 0.9211 - val_loss: 0.2425 - val_accuracy: 0.9342\n",
      "Epoch 4/10\n",
      "1094/1094 [==============================] - 4s 3ms/step - loss: 0.2397 - accuracy: 0.9309 - val_loss: 0.2205 - val_accuracy: 0.9396\n",
      "Epoch 5/10\n",
      "1094/1094 [==============================] - 4s 3ms/step - loss: 0.2134 - accuracy: 0.9390 - val_loss: 0.1963 - val_accuracy: 0.9436\n",
      "Epoch 6/10\n",
      "1094/1094 [==============================] - 4s 3ms/step - loss: 0.1919 - accuracy: 0.9456 - val_loss: 0.1835 - val_accuracy: 0.9498\n",
      "Epoch 7/10\n",
      "1094/1094 [==============================] - 4s 3ms/step - loss: 0.1742 - accuracy: 0.9495 - val_loss: 0.1676 - val_accuracy: 0.9544\n",
      "Epoch 8/10\n",
      "1094/1094 [==============================] - 4s 3ms/step - loss: 0.1593 - accuracy: 0.9541 - val_loss: 0.1589 - val_accuracy: 0.9574\n",
      "Epoch 9/10\n",
      "1094/1094 [==============================] - 4s 3ms/step - loss: 0.1459 - accuracy: 0.9581 - val_loss: 0.1482 - val_accuracy: 0.9614\n",
      "Epoch 10/10\n",
      "1094/1094 [==============================] - 4s 3ms/step - loss: 0.1341 - accuracy: 0.9622 - val_loss: 0.1407 - val_accuracy: 0.9624\n",
      "time : 36.32639408111572\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1451 - accuracy: 0.9571\n",
      "1/1 [==============================] - 0s 65ms/step\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # train:test = 80:20\n",
    "    X_train_full, y_train_full, X_test, y_test = load_data()\n",
    "    \n",
    "    print('0~255 Ver\\n') # divide를 1로 설정하여 0~255 그대로 유지\n",
    "    X_valid, X_train, y_valid, y_train, x_test = data_normalization(X_train_full, y_train_full, X_test, 1.)\n",
    "    model_255, history_255 = makemodel(X_train, y_train, X_valid, y_valid, 'glorot_uniform')\n",
    "    evalmodel(model_255, history_255, x_test, y_test)\n",
    "    \n",
    "    print('\\n 0~1 정규화 Ver\\n') # divide를 255로 설정하여 0~1로 정규화\n",
    "    X_valid, X_train, y_valid, y_train, x_test = data_normalization(X_train_full, y_train_full, X_test, 255.)\n",
    "    model_normalization, history_normalization = makemodel(X_train, y_train, X_valid, y_valid, 'glorot_uniform')\n",
    "    evalmodel(model_normalization, history_normalization, x_test, y_test)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 정규화 여부 결과\n",
    " 0~255로 정규화하지 않은 경우와 0~1로 정규화한 경우를 비교해보면  \n",
    " 극심할 정도로 정규화를 진행한 쪽이 모든 데이터의 전체 학습 기간에 대해 loss가 낮고, accuracy는 높다.  \n",
    " 에포크가 10으로 작지만, time도 정규화를 진행한 쪽이 살짝 덜 걸렸다.  \n",
    " 정리하면, 전체적으로 0~1로 정규화를 진행한 쪽이 학습에 매우 효과적이었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. dropout 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout 미적용 Ver\n",
      "\n",
      "Epoch 1/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.7709 - accuracy: 0.8084 - val_loss: 0.3569 - val_accuracy: 0.9050\n",
      "Epoch 2/50\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 0.3362 - accuracy: 0.9046 - val_loss: 0.2847 - val_accuracy: 0.9206\n",
      "Epoch 3/50\n",
      "1094/1094 [==============================] - 5s 4ms/step - loss: 0.2768 - accuracy: 0.9204 - val_loss: 0.2380 - val_accuracy: 0.9352\n",
      "Epoch 4/50\n",
      "1094/1094 [==============================] - 5s 4ms/step - loss: 0.2417 - accuracy: 0.9312 - val_loss: 0.2128 - val_accuracy: 0.9438\n",
      "Epoch 5/50\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 0.2151 - accuracy: 0.9388 - val_loss: 0.1935 - val_accuracy: 0.9486\n",
      "Epoch 6/50\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 0.1939 - accuracy: 0.9441 - val_loss: 0.1787 - val_accuracy: 0.9522\n",
      "Epoch 7/50\n",
      "1094/1094 [==============================] - 5s 4ms/step - loss: 0.1762 - accuracy: 0.9493 - val_loss: 0.1630 - val_accuracy: 0.9546\n",
      "Epoch 8/50\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 0.1614 - accuracy: 0.9534 - val_loss: 0.1564 - val_accuracy: 0.9578\n",
      "Epoch 9/50\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 0.1480 - accuracy: 0.9579 - val_loss: 0.1519 - val_accuracy: 0.9586\n",
      "Epoch 10/50\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 0.1362 - accuracy: 0.9619 - val_loss: 0.1392 - val_accuracy: 0.9622\n",
      "Epoch 11/50\n",
      "1094/1094 [==============================] - 5s 4ms/step - loss: 0.1269 - accuracy: 0.9646 - val_loss: 0.1310 - val_accuracy: 0.9642\n",
      "Epoch 12/50\n",
      "1094/1094 [==============================] - 5s 4ms/step - loss: 0.1183 - accuracy: 0.9673 - val_loss: 0.1260 - val_accuracy: 0.9652\n",
      "Epoch 13/50\n",
      "1094/1094 [==============================] - 5s 4ms/step - loss: 0.1099 - accuracy: 0.9697 - val_loss: 0.1208 - val_accuracy: 0.9666\n",
      "Epoch 14/50\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 0.1030 - accuracy: 0.9716 - val_loss: 0.1160 - val_accuracy: 0.9678\n",
      "Epoch 15/50\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 0.0963 - accuracy: 0.9739 - val_loss: 0.1112 - val_accuracy: 0.9686\n",
      "Epoch 16/50\n",
      "1094/1094 [==============================] - 5s 4ms/step - loss: 0.0903 - accuracy: 0.9752 - val_loss: 0.1120 - val_accuracy: 0.9706\n",
      "Epoch 17/50\n",
      "1094/1094 [==============================] - 5s 4ms/step - loss: 0.0849 - accuracy: 0.9765 - val_loss: 0.1066 - val_accuracy: 0.9712\n",
      "Epoch 18/50\n",
      "1094/1094 [==============================] - 5s 4ms/step - loss: 0.0803 - accuracy: 0.9781 - val_loss: 0.1022 - val_accuracy: 0.9716\n",
      "Epoch 19/50\n",
      "1094/1094 [==============================] - 5s 4ms/step - loss: 0.0756 - accuracy: 0.9797 - val_loss: 0.1016 - val_accuracy: 0.9726\n",
      "Epoch 20/50\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 0.0713 - accuracy: 0.9813 - val_loss: 0.1000 - val_accuracy: 0.9724\n",
      "Epoch 21/50\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 0.0675 - accuracy: 0.9826 - val_loss: 0.0953 - val_accuracy: 0.9736\n",
      "Epoch 22/50\n",
      "1094/1094 [==============================] - 5s 4ms/step - loss: 0.0635 - accuracy: 0.9841 - val_loss: 0.0947 - val_accuracy: 0.9736\n",
      "Epoch 23/50\n",
      "1094/1094 [==============================] - 5s 4ms/step - loss: 0.0604 - accuracy: 0.9848 - val_loss: 0.0945 - val_accuracy: 0.9724\n",
      "Epoch 24/50\n",
      "1094/1094 [==============================] - 5s 4ms/step - loss: 0.0570 - accuracy: 0.9856 - val_loss: 0.0903 - val_accuracy: 0.9752\n",
      "Epoch 25/50\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 0.0541 - accuracy: 0.9866 - val_loss: 0.0887 - val_accuracy: 0.9756\n",
      "Epoch 26/50\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 0.0513 - accuracy: 0.9874 - val_loss: 0.0883 - val_accuracy: 0.9756\n",
      "Epoch 27/50\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 0.0487 - accuracy: 0.9876 - val_loss: 0.0866 - val_accuracy: 0.9746\n",
      "Epoch 28/50\n",
      "1094/1094 [==============================] - 5s 4ms/step - loss: 0.0459 - accuracy: 0.9887 - val_loss: 0.0856 - val_accuracy: 0.9762\n",
      "Epoch 29/50\n",
      "1094/1094 [==============================] - 5s 4ms/step - loss: 0.0438 - accuracy: 0.9896 - val_loss: 0.0841 - val_accuracy: 0.9758\n",
      "Epoch 30/50\n",
      "1094/1094 [==============================] - 5s 4ms/step - loss: 0.0414 - accuracy: 0.9905 - val_loss: 0.0836 - val_accuracy: 0.9762\n",
      "Epoch 31/50\n",
      "1094/1094 [==============================] - 5s 4ms/step - loss: 0.0394 - accuracy: 0.9909 - val_loss: 0.0846 - val_accuracy: 0.9768\n",
      "Epoch 32/50\n",
      "1094/1094 [==============================] - 5s 4ms/step - loss: 0.0375 - accuracy: 0.9917 - val_loss: 0.0811 - val_accuracy: 0.9766\n",
      "Epoch 33/50\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 0.0356 - accuracy: 0.9921 - val_loss: 0.0806 - val_accuracy: 0.9770\n",
      "Epoch 34/50\n",
      "1094/1094 [==============================] - 5s 4ms/step - loss: 0.0337 - accuracy: 0.9929 - val_loss: 0.0801 - val_accuracy: 0.9766\n",
      "Epoch 35/50\n",
      "1094/1094 [==============================] - 5s 4ms/step - loss: 0.0320 - accuracy: 0.9931 - val_loss: 0.0797 - val_accuracy: 0.9770\n",
      "Epoch 36/50\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 0.0305 - accuracy: 0.9937 - val_loss: 0.0791 - val_accuracy: 0.9764\n",
      "Epoch 37/50\n",
      "1094/1094 [==============================] - 5s 4ms/step - loss: 0.0292 - accuracy: 0.9942 - val_loss: 0.0789 - val_accuracy: 0.9770\n",
      "Epoch 38/50\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 0.0276 - accuracy: 0.9946 - val_loss: 0.0782 - val_accuracy: 0.9770\n",
      "Epoch 39/50\n",
      "1094/1094 [==============================] - 5s 4ms/step - loss: 0.0260 - accuracy: 0.9953 - val_loss: 0.0790 - val_accuracy: 0.9766\n",
      "Epoch 40/50\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 0.0253 - accuracy: 0.9951 - val_loss: 0.0794 - val_accuracy: 0.9762\n",
      "Epoch 41/50\n",
      "1094/1094 [==============================] - 5s 4ms/step - loss: 0.0237 - accuracy: 0.9960 - val_loss: 0.0784 - val_accuracy: 0.9766\n",
      "Epoch 42/50\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 0.0226 - accuracy: 0.9964 - val_loss: 0.0781 - val_accuracy: 0.9772\n",
      "Epoch 43/50\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 0.0216 - accuracy: 0.9966 - val_loss: 0.0770 - val_accuracy: 0.9768\n",
      "Epoch 44/50\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 0.0206 - accuracy: 0.9969 - val_loss: 0.0815 - val_accuracy: 0.9760\n",
      "Epoch 45/50\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 0.0198 - accuracy: 0.9972 - val_loss: 0.0765 - val_accuracy: 0.9768\n",
      "Epoch 46/50\n",
      "1094/1094 [==============================] - 4s 4ms/step - loss: 0.0189 - accuracy: 0.9973 - val_loss: 0.0769 - val_accuracy: 0.9770\n",
      "Epoch 47/50\n",
      "1094/1094 [==============================] - 5s 4ms/step - loss: 0.0180 - accuracy: 0.9975 - val_loss: 0.0765 - val_accuracy: 0.9772\n",
      "Epoch 48/50\n",
      "1094/1094 [==============================] - 5s 4ms/step - loss: 0.0171 - accuracy: 0.9979 - val_loss: 0.0773 - val_accuracy: 0.9770\n",
      "Epoch 49/50\n",
      "1094/1094 [==============================] - 5s 4ms/step - loss: 0.0164 - accuracy: 0.9981 - val_loss: 0.0772 - val_accuracy: 0.9770\n",
      "Epoch 50/50\n",
      "1094/1094 [==============================] - 5s 4ms/step - loss: 0.0158 - accuracy: 0.9982 - val_loss: 0.0853 - val_accuracy: 0.9762\n",
      "time : 227.20604634284973\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0952 - accuracy: 0.9742\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "\n",
      " Dropout 적용 Ver\n",
      "\n",
      "Epoch 1/50\n",
      "1094/1094 [==============================] - 6s 5ms/step - loss: 0.9342 - accuracy: 0.7285 - val_loss: 0.3875 - val_accuracy: 0.8998\n",
      "Epoch 2/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.4444 - accuracy: 0.8704 - val_loss: 0.2902 - val_accuracy: 0.9192\n",
      "Epoch 3/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.3556 - accuracy: 0.8962 - val_loss: 0.2447 - val_accuracy: 0.9310\n",
      "Epoch 4/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.3046 - accuracy: 0.9112 - val_loss: 0.2161 - val_accuracy: 0.9386\n",
      "Epoch 5/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.2743 - accuracy: 0.9197 - val_loss: 0.1957 - val_accuracy: 0.9432\n",
      "Epoch 6/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.2492 - accuracy: 0.9256 - val_loss: 0.1779 - val_accuracy: 0.9474\n",
      "Epoch 7/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.2291 - accuracy: 0.9325 - val_loss: 0.1647 - val_accuracy: 0.9538\n",
      "Epoch 8/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.2101 - accuracy: 0.9390 - val_loss: 0.1541 - val_accuracy: 0.9560\n",
      "Epoch 9/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.1955 - accuracy: 0.9429 - val_loss: 0.1450 - val_accuracy: 0.9580\n",
      "Epoch 10/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.1830 - accuracy: 0.9461 - val_loss: 0.1385 - val_accuracy: 0.9582\n",
      "Epoch 11/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.1720 - accuracy: 0.9494 - val_loss: 0.1313 - val_accuracy: 0.9608\n",
      "Epoch 12/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.1627 - accuracy: 0.9521 - val_loss: 0.1270 - val_accuracy: 0.9630\n",
      "Epoch 13/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.1543 - accuracy: 0.9544 - val_loss: 0.1229 - val_accuracy: 0.9634\n",
      "Epoch 14/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.1457 - accuracy: 0.9573 - val_loss: 0.1169 - val_accuracy: 0.9674\n",
      "Epoch 15/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.1393 - accuracy: 0.9590 - val_loss: 0.1114 - val_accuracy: 0.9682\n",
      "Epoch 16/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.1359 - accuracy: 0.9602 - val_loss: 0.1076 - val_accuracy: 0.9674\n",
      "Epoch 17/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.1255 - accuracy: 0.9632 - val_loss: 0.1051 - val_accuracy: 0.9686\n",
      "Epoch 18/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.1232 - accuracy: 0.9630 - val_loss: 0.1012 - val_accuracy: 0.9696\n",
      "Epoch 19/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.1147 - accuracy: 0.9661 - val_loss: 0.1004 - val_accuracy: 0.9708\n",
      "Epoch 20/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.1102 - accuracy: 0.9673 - val_loss: 0.0984 - val_accuracy: 0.9730\n",
      "Epoch 21/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.1073 - accuracy: 0.9684 - val_loss: 0.0953 - val_accuracy: 0.9726\n",
      "Epoch 22/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.1015 - accuracy: 0.9704 - val_loss: 0.0939 - val_accuracy: 0.9720\n",
      "Epoch 23/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.0971 - accuracy: 0.9717 - val_loss: 0.0925 - val_accuracy: 0.9732\n",
      "Epoch 24/50\n",
      "1094/1094 [==============================] - 6s 5ms/step - loss: 0.0974 - accuracy: 0.9713 - val_loss: 0.0900 - val_accuracy: 0.9742\n",
      "Epoch 25/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.0936 - accuracy: 0.9717 - val_loss: 0.0872 - val_accuracy: 0.9750\n",
      "Epoch 26/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.0886 - accuracy: 0.9748 - val_loss: 0.0874 - val_accuracy: 0.9744\n",
      "Epoch 27/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.0860 - accuracy: 0.9749 - val_loss: 0.0852 - val_accuracy: 0.9746\n",
      "Epoch 28/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.0829 - accuracy: 0.9756 - val_loss: 0.0839 - val_accuracy: 0.9758\n",
      "Epoch 29/50\n",
      "1094/1094 [==============================] - 6s 5ms/step - loss: 0.0788 - accuracy: 0.9759 - val_loss: 0.0823 - val_accuracy: 0.9762\n",
      "Epoch 30/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.0782 - accuracy: 0.9766 - val_loss: 0.0816 - val_accuracy: 0.9770\n",
      "Epoch 31/50\n",
      "1094/1094 [==============================] - 6s 5ms/step - loss: 0.0757 - accuracy: 0.9775 - val_loss: 0.0797 - val_accuracy: 0.9762\n",
      "Epoch 32/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.0741 - accuracy: 0.9779 - val_loss: 0.0790 - val_accuracy: 0.9764\n",
      "Epoch 33/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.0707 - accuracy: 0.9801 - val_loss: 0.0773 - val_accuracy: 0.9768\n",
      "Epoch 34/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.0704 - accuracy: 0.9793 - val_loss: 0.0780 - val_accuracy: 0.9774\n",
      "Epoch 35/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.0681 - accuracy: 0.9796 - val_loss: 0.0779 - val_accuracy: 0.9780\n",
      "Epoch 36/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.0659 - accuracy: 0.9800 - val_loss: 0.0760 - val_accuracy: 0.9784\n",
      "Epoch 37/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.0638 - accuracy: 0.9809 - val_loss: 0.0751 - val_accuracy: 0.9778\n",
      "Epoch 38/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.0631 - accuracy: 0.9814 - val_loss: 0.0751 - val_accuracy: 0.9770\n",
      "Epoch 39/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.0611 - accuracy: 0.9815 - val_loss: 0.0743 - val_accuracy: 0.9774\n",
      "Epoch 40/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.0581 - accuracy: 0.9828 - val_loss: 0.0724 - val_accuracy: 0.9780\n",
      "Epoch 41/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.0553 - accuracy: 0.9834 - val_loss: 0.0742 - val_accuracy: 0.9772\n",
      "Epoch 42/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.0565 - accuracy: 0.9833 - val_loss: 0.0734 - val_accuracy: 0.9786\n",
      "Epoch 43/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.0538 - accuracy: 0.9842 - val_loss: 0.0723 - val_accuracy: 0.9778\n",
      "Epoch 44/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.0509 - accuracy: 0.9848 - val_loss: 0.0728 - val_accuracy: 0.9784\n",
      "Epoch 45/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.0499 - accuracy: 0.9857 - val_loss: 0.0702 - val_accuracy: 0.9790\n",
      "Epoch 46/50\n",
      "1094/1094 [==============================] - 5s 4ms/step - loss: 0.0481 - accuracy: 0.9865 - val_loss: 0.0710 - val_accuracy: 0.9790\n",
      "Epoch 47/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.0477 - accuracy: 0.9857 - val_loss: 0.0708 - val_accuracy: 0.9792\n",
      "Epoch 48/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.0459 - accuracy: 0.9858 - val_loss: 0.0709 - val_accuracy: 0.9790\n",
      "Epoch 49/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.0450 - accuracy: 0.9867 - val_loss: 0.0705 - val_accuracy: 0.9794\n",
      "Epoch 50/50\n",
      "1094/1094 [==============================] - 5s 5ms/step - loss: 0.0454 - accuracy: 0.9869 - val_loss: 0.0706 - val_accuracy: 0.9784\n",
      "time : 259.8462555408478\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0731 - accuracy: 0.9782\n",
      "1/1 [==============================] - 0s 55ms/step\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # train:test = 80:20\n",
    "    X_train_full, y_train_full, X_test, y_test = load_data()\n",
    "    \n",
    "    print('Dropout 미적용 Ver\\n') # 에포크 50\n",
    "    X_valid, X_train, y_valid, y_train, x_test = data_normalization(X_train_full, y_train_full, X_test, 255.)\n",
    "    model_normal, history_normal = makemodel(X_train, y_train, X_valid, y_valid, 'glorot_uniform', False, 50)\n",
    "    evalmodel(model_normal, history_normal, x_test, y_test)\n",
    "    \n",
    "    print('\\n Dropout 적용 Ver\\n') # 에포크 50\n",
    "    X_valid, X_train, y_valid, y_train, x_test = data_normalization(X_train_full, y_train_full, X_test, 255.)\n",
    "    model_dropout, history_dropout = makemodel(X_train, y_train, X_valid, y_valid, 'glorot_uniform', True, 50)\n",
    "    evalmodel(model_dropout, history_dropout, x_test, y_test)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. dropout 여부 결과\n",
    "dropout을 진행한 경우 에포크가 낮은 초기 단계에서  \n",
    "학습하는 뉴런 수가 줄기에 학습 속도와 효율이 떨어진다.  \n",
    "즉 초기에서는 dropout을 적용하지 않은 경우에 비해  \n",
    "loss가 높고 정확도가 낮아지는 결과를 보인다.  \n",
    "다시 정리하면, 과잉적합 되지 않는 에포크 수가 낮은 경우에는  \n",
    "오히려 드롭아웃을 사용하지 않거나 비율을 낮게 하는 것이 결과가 더 좋게 나온다.  \n",
    "dropout은 모델이 과잉적합되어 일반화 성능이 떨어질 때 효과적이다.  \n",
    "즉 모델이 어느정도 epoch 정도부터 과잉적합이 되는지 정확히 파악하지  \n",
    "못하는 학습 진행 단계에서 epoch를 크게 돌려놓고, dropout을 적용해  \n",
    "학습을 진행하며 과잉적합되는 시기를 늦추며 더 좋은 성능의  \n",
    "모델을 결과를 얻을 수 있다.  \n",
    "\n",
    "아래 결과는 50에포크를 수행한 결과이다.  \n",
    "dropout을 적용한 경우에 val, test 데이터에 대해 적용하지 않은 경우에 비해  \n",
    "loss가 낮고, acc가 소폭 높은 것을 확인할 수 있다. \n",
    "즉 일반화 성능이 dropout을 적용했을 때 더 좋다.  \n",
    "추가로 train의 loss와 acc를 살펴봤을 때 dropout을 적용하지 않은 경우  \n",
    "train 데이터에 대해 너무 잘 학습되어지는 과잉적합이 의심되는 단계에 있음을  \n",
    "알 수 있다. dropout을 적용한 경우는 덜 과잉적합 되었음을 알 수 있다.  \n",
    "\n",
    "결과적으로 위에서 말했던 것처럼 epoch가 커질경우 과잉적합을  \n",
    "방지하기 위해 dropout을 사용하는 것이 학습성능에 더 효과적이다.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dropout 미적용\n",
    "1094/1094 [==============================] - 5s 4ms/step - loss: 0.0158 - accuracy: 0.9982 - val_loss: 0.0853 - val_accuracy: 0.9762\n",
    "time : 227.20604634284973\n",
    "313/313 [==============================] - 1s 3ms/step - loss: 0.0952 - accuracy: 0.9742\n",
    "1/1 [==============================] - 0s 55ms/step  \n",
    "\n",
    "\n",
    "\n",
    "##### dropout 적용\n",
    "1094/1094 [==============================] - 5s 5ms/step - loss: 0.0454 - accuracy: 0.9869 - val_loss: 0.0706 - val_accuracy: 0.9784\n",
    "time : 259.8462555408478\n",
    "313/313 [==============================] - 1s 3ms/step - loss: 0.0731 - accuracy: 0.9782\n",
    "1/1 [==============================] - 0s 55ms/step"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
